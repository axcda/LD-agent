#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¼˜åŒ–ç‰ˆè®ºå›æ•°æ®åˆ†æå™¨
é¢„å¤„ç†æ•°æ®åæ‰¹é‡åˆ†æï¼Œé¿å…æ¯ä¸ªå¸–å­éƒ½è°ƒç”¨å¤§æ¨¡å‹
"""

import json
import sys
import re
import requests
from typing import Dict, Any, List, Tuple
from urllib.parse import urlparse


class ForumDataPreprocessor:
    """è®ºå›æ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self):
        self.image_patterns = [
            r'https?://[^\s]+\.(jpg|jpeg|png|gif|webp)',
            r'!\[.*?\]\([^\)]+\)',  # markdownå›¾ç‰‡
        ]
        self.url_patterns = [
            r'https?://[^\s]+',
        ]
    
    def extract_links_and_images(self, text: str) -> Tuple[List[str], List[str]]:
        """ä»æ–‡æœ¬ä¸­æå–é“¾æ¥å’Œå›¾ç‰‡"""
        images = []
        links = []
        
        # æå–å›¾ç‰‡
        for pattern in self.image_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            images.extend(matches)
        
        # æå–é“¾æ¥
        for pattern in self.url_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                # æ’é™¤å·²ç»è¯†åˆ«ä¸ºå›¾ç‰‡çš„é“¾æ¥
                if not any(img_ext in match.lower() for img_ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']):
                    links.append(match)
        
        return list(set(links)), list(set(images))
    
    def preprocess_forum_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """é¢„å¤„ç†è®ºå›æ•°æ®"""
        processed_data = {
            "topic_info": {
                "title": data.get("topicTitle", ""),
                "url": data.get("url", ""),
                "timestamp": data.get("timestamp", ""),
                "total_posts": data.get("totalPosts", 0)
            },
            "content_summary": {
                "main_discussion": "",
                "key_users": set(),
                "all_links": [],
                "all_images": [],
                "post_count": 0
            },
            "structured_content": []
        }
        
        posts = data.get("posts", [])
        processed_data["content_summary"]["post_count"] = len(posts)
        
        # å¤„ç†æ¯ä¸ªå¸–å­
        for i, post in enumerate(posts):
            username = post.get("username", "æœªçŸ¥ç”¨æˆ·")
            text_content = post.get("content", {}).get("text", "")
            post_images = post.get("content", {}).get("images", [])
            post_links_data = post.get("content", {}).get("links", [])
            
            # è®°å½•å…³é”®ç”¨æˆ·
            processed_data["content_summary"]["key_users"].add(username)
            
            # å¤„ç†é“¾æ¥æ•°æ®ï¼ˆä»å­—å…¸æ ¼å¼æå–hrefï¼‰
            post_links = []
            for link_item in post_links_data:
                if isinstance(link_item, dict) and "href" in link_item:
                    post_links.append(link_item["href"])
                elif isinstance(link_item, str):
                    post_links.append(link_item)
            
            # æå–é¢å¤–çš„é“¾æ¥å’Œå›¾ç‰‡
            extracted_links, extracted_images = self.extract_links_and_images(text_content)
            
            # åˆå¹¶æ‰€æœ‰é“¾æ¥å’Œå›¾ç‰‡
            all_post_links = list(set(post_links + extracted_links))
            all_post_images = list(set(post_images + extracted_images))
            
            processed_data["content_summary"]["all_links"].extend(all_post_links)
            processed_data["content_summary"]["all_images"].extend(all_post_images)
            
            # æ„å»ºç»“æ„åŒ–å†…å®¹
            processed_post = {
                "index": i + 1,
                "username": username,
                "content": text_content,
                "has_media": len(all_post_images) > 0,
                "has_links": len(all_post_links) > 0,
                "media_count": len(all_post_images),
                "links_count": len(all_post_links)
            }
            processed_data["structured_content"].append(processed_post)
            
            # æ„å»ºä¸»è¦è®¨è®ºå†…å®¹ï¼ˆé™åˆ¶é•¿åº¦ï¼‰
            if len(processed_data["content_summary"]["main_discussion"]) < 2000:
                processed_data["content_summary"]["main_discussion"] += f"[{username}]: {text_content[:200]}...\n"
        
        # å»é‡å’Œæ•´ç†
        processed_data["content_summary"]["key_users"] = list(processed_data["content_summary"]["key_users"])
        processed_data["content_summary"]["all_links"] = list(set(processed_data["content_summary"]["all_links"]))
        processed_data["content_summary"]["all_images"] = list(set(processed_data["content_summary"]["all_images"]))
        
        return processed_data


class OptimizedForumAnalyzer:
    """ä¼˜åŒ–ç‰ˆè®ºå›åˆ†æå™¨"""
    
    def __init__(self, api_base_url: str = "http://localhost:9980"):
        self.api_base_url = api_base_url
        self.preprocessor = ForumDataPreprocessor()
    
    def check_api_health(self) -> bool:
        """æ£€æŸ¥APIå¥åº·çŠ¶æ€"""
        try:
            response = requests.get(f"{self.api_base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def create_analysis_content(self, processed_data: Dict[str, Any]) -> str:
        """åˆ›å»ºåˆ†æå†…å®¹"""
        topic_info = processed_data["topic_info"]
        summary = processed_data["content_summary"]
        
        content = f"""è®ºå›ä¸»é¢˜åˆ†ææŠ¥å‘Š
================
ä¸»é¢˜: {topic_info['title']}
é“¾æ¥: {topic_info['url']}
å‘å¸ƒæ—¶é—´: {topic_info['timestamp']}
æ€»å›å¤æ•°: {summary['post_count']}

å‚ä¸ç”¨æˆ·: {', '.join(summary['key_users'][:10])}{'...' if len(summary['key_users']) > 10 else ''}
åª’ä½“å†…å®¹: {len(summary['all_images'])}å¼ å›¾ç‰‡, {len(summary['all_links'])}ä¸ªé“¾æ¥

ä¸»è¦è®¨è®ºå†…å®¹:
{summary['main_discussion']}

éœ€è¦è¿›ä¸€æ­¥åˆ†æçš„åª’ä½“å†…å®¹:
"""
        
        # æ·»åŠ éœ€è¦åˆ†æçš„é“¾æ¥
        if summary['all_links']:
            content += f"\nå¤–éƒ¨é“¾æ¥({len(summary['all_links'])}ä¸ª):\n"
            for i, link in enumerate(summary['all_links'][:5]):  # åªåˆ—å‡ºå‰5ä¸ª
                content += f"  {i+1}. {link}\n"
            if len(summary['all_links']) > 5:
                content += f"  ... è¿˜æœ‰{len(summary['all_links'])-5}ä¸ªé“¾æ¥\n"
        
        # æ·»åŠ éœ€è¦åˆ†æçš„å›¾ç‰‡
        if summary['all_images']:
            content += f"\nå›¾ç‰‡å†…å®¹({len(summary['all_images'])}å¼ ):\n"
            for i, img in enumerate(summary['all_images'][:3]):  # åªåˆ—å‡ºå‰3ä¸ª
                content += f"  {i+1}. {img}\n"
            if len(summary['all_images']) > 3:
                content += f"  ... è¿˜æœ‰{len(summary['all_images'])-3}å¼ å›¾ç‰‡\n"
        
        return content
    
    def analyze_media_content(self, processed_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åˆ†æåª’ä½“å†…å®¹ï¼Œä½¿ç”¨å¯¹åº”çš„workflow"""
        analysis_requests = []
        summary = processed_data["content_summary"]
        
        # åˆ†æé‡è¦é“¾æ¥ - ä½¿ç”¨URL workflow
        for i, link in enumerate(summary['all_links'][:3]):  # åªåˆ†æå‰3ä¸ªé“¾æ¥
            # éªŒè¯é“¾æ¥æ ¼å¼
            if self._is_valid_url(link):
                analysis_requests.append({
                    "content": link,
                    "content_type": "url",
                    "context": f"è®ºå›è®¨è®ºä¸­çš„å¤–éƒ¨é“¾æ¥ #{i+1}: {processed_data['topic_info']['title']}"
                })
        
        # åˆ†æé‡è¦å›¾ç‰‡ - ä½¿ç”¨Image workflow  
        for i, img_url in enumerate(summary['all_images'][:2]):  # åªåˆ†æå‰2å¼ å›¾ç‰‡
            # éªŒè¯å›¾ç‰‡é“¾æ¥æ ¼å¼
            if self._is_valid_image_url(img_url):
                analysis_requests.append({
                    "content": img_url,
                    "content_type": "image", 
                    "context": f"è®ºå›è®¨è®ºä¸­çš„å›¾ç‰‡ #{i+1}: {processed_data['topic_info']['title']}"
                })
        
        return analysis_requests
    
    def _is_valid_url(self, url: str) -> bool:
        """éªŒè¯URLæ ¼å¼"""
        try:
            from urllib.parse import urlparse
            result = urlparse(url)
            return all([result.scheme, result.netloc])
        except:
            return False
    
    def _is_valid_image_url(self, url: str) -> bool:
        """éªŒè¯å›¾ç‰‡URLæ ¼å¼"""
        if not self._is_valid_url(url):
            return False
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå›¾ç‰‡æ‰©å±•å
        image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.bmp']
        url_lower = url.lower()
        
        # ç›´æ¥åŒ…å«å›¾ç‰‡æ‰©å±•å
        if any(ext in url_lower for ext in image_extensions):
            return True
            
        # æˆ–è€…æ˜¯å›¾ç‰‡æœåŠ¡åŸŸå (å¦‚uploads.linux.doç­‰)
        image_domains = ['uploads.', 'images.', 'img.', 'cdn.']
        if any(domain in url_lower for domain in image_domains):
            return True
            
        return False
    
    def perform_analysis(self, forum_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œä¼˜åŒ–çš„åˆ†ææµç¨‹"""
        print("ğŸ”„ é¢„å¤„ç†è®ºå›æ•°æ®...")
        processed_data = self.preprocessor.preprocess_forum_data(forum_data)
        
        print(f"âœ… é¢„å¤„ç†å®Œæˆ:")
        print(f"  - ä¸»é¢˜: {processed_data['topic_info']['title']}")
        print(f"  - å¸–å­æ•°: {processed_data['content_summary']['post_count']}")
        print(f"  - å‚ä¸ç”¨æˆ·: {len(processed_data['content_summary']['key_users'])}äºº")
        print(f"  - é“¾æ¥: {len(processed_data['content_summary']['all_links'])}ä¸ª")
        print(f"  - å›¾ç‰‡: {len(processed_data['content_summary']['all_images'])}å¼ ")
        
        # 1. ä¸»è¦å†…å®¹åˆ†æ
        print("\nğŸ” åˆ†æä¸»è¦è®¨è®ºå†…å®¹...")
        main_content = self.create_analysis_content(processed_data)
        
        main_analysis_payload = {
            "content": main_content,
            "content_type": "text",
            "context": f"è®ºå›ä¸»é¢˜åˆ†æ: {processed_data['topic_info']['title']}"
        }
        
        try:
            response = requests.post(f"{self.api_base_url}/analyze", json=main_analysis_payload, timeout=30)
            if response.status_code == 200:
                main_result = response.json()
                if main_result.get("success"):
                    print("âœ… ä¸»è¦å†…å®¹åˆ†æå®Œæˆ")
                    analysis = main_result["data"]["analysis"]
                    print(f"æ‘˜è¦: {analysis['summary'][:150]}...")
                    print("å…³é”®ç‚¹:")
                    for i, point in enumerate(analysis["key_points"][:3], 1):
                        print(f"  {i}. {point}")
                else:
                    print(f"âŒ ä¸»è¦å†…å®¹åˆ†æå¤±è´¥: {main_result.get('error', {}).get('message')}")
                    return main_result
            else:
                print(f"âŒ APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                return {"success": False, "error": {"message": f"HTTP {response.status_code}"}}
        except Exception as e:
            print(f"âŒ è¯·æ±‚å‡ºé”™: {str(e)}")
            return {"success": False, "error": {"message": str(e)}}
        
        # 2. åª’ä½“å†…å®¹åˆ†æï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        media_requests = self.analyze_media_content(processed_data)
        if media_requests:
            print(f"\nğŸ–¼ï¸ åˆ†æåª’ä½“å†…å®¹({len(media_requests)}é¡¹)...")
            
            batch_payload = {"requests": media_requests}
            
            try:
                response = requests.post(f"{self.api_base_url}/analyze/batch", json=batch_payload, timeout=60)
                if response.status_code == 200:
                    media_result = response.json()
                    if media_result.get("success"):
                        print("âœ… åª’ä½“å†…å®¹åˆ†æå®Œæˆ")
                        media_analysis = media_result["data"]["analysis"]
                        print(f"åª’ä½“åˆ†ææ‘˜è¦: {media_analysis['summary'][:150]}...")
                        
                        # åˆå¹¶åˆ†æç»“æœ
                        combined_summary = f"{analysis['summary']}\n\nåª’ä½“å†…å®¹è¡¥å……: {media_analysis['summary']}"
                        combined_key_points = analysis["key_points"] + media_analysis["key_points"]
                        
                        return {
                            "success": True,
                            "data": {
                                "analysis": {
                                    "summary": combined_summary,
                                    "key_points": combined_key_points[:10],  # é™åˆ¶å…³é”®ç‚¹æ•°é‡
                                    "processed_info": {
                                        "total_posts": processed_data['content_summary']['post_count'],
                                        "users_count": len(processed_data['content_summary']['key_users']),
                                        "links_count": len(processed_data['content_summary']['all_links']),
                                        "images_count": len(processed_data['content_summary']['all_images'])
                                    }
                                }
                            }
                        }
                    else:
                        print(f"âš ï¸ åª’ä½“å†…å®¹åˆ†æå¤±è´¥ï¼Œä»…è¿”å›ä¸»è¦å†…å®¹åˆ†æ")
                else:
                    print(f"âš ï¸ åª’ä½“åˆ†æAPIè¯·æ±‚å¤±è´¥: {response.status_code}")
            except Exception as e:
                print(f"âš ï¸ åª’ä½“åˆ†æè¯·æ±‚å‡ºé”™: {str(e)}")
        
        # è¿”å›ä¸»è¦åˆ†æç»“æœ
        return main_result


def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ–¹æ³•: python optimized_forum_analyzer.py <jsonæ–‡ä»¶è·¯å¾„>")
        print("ç¤ºä¾‹: python optimized_forum_analyzer.py sample_forum_data.json")
        return
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = OptimizedForumAnalyzer()
    
    # æ£€æŸ¥APIçŠ¶æ€
    if not analyzer.check_api_health():
        print("âŒ æ— æ³•è¿æ¥åˆ°APIæœåŠ¡å™¨ï¼Œè¯·ç¡®ä¿æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ")
        return
    
    print("âœ… è¿æ¥åˆ°APIæœåŠ¡å™¨")
    
    # åŠ è½½æ•°æ®
    try:
        with open(sys.argv[1], 'r', encoding='utf-8') as f:
            forum_data = json.load(f)
        print(f"âœ… åŠ è½½æ•°æ®æˆåŠŸ: {forum_data.get('topicTitle', '')}")
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {str(e)}")
        return
    
    # æ‰§è¡Œåˆ†æ
    print("\n" + "="*60)
    print("ğŸš€ å¼€å§‹ä¼˜åŒ–åˆ†æ...")
    
    result = analyzer.perform_analysis(forum_data)
    
    print("\n" + "="*60)
    if result.get("success"):
        print("ğŸ‰ åˆ†æå®Œæˆ!")
        if "processed_info" in result["data"]["analysis"]:
            info = result["data"]["analysis"]["processed_info"]
            print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
            print(f"  - æ€»å¸–å­æ•°: {info['total_posts']}")
            print(f"  - å‚ä¸ç”¨æˆ·: {info['users_count']}äºº")
            print(f"  - å¤–éƒ¨é“¾æ¥: {info['links_count']}ä¸ª")
            print(f"  - å›¾ç‰‡å†…å®¹: {info['images_count']}å¼ ")
    else:
        print("âŒ åˆ†æå¤±è´¥")


if __name__ == "__main__":
    main()