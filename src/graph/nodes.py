from typing import Dict, Any, List
from src.graph.state import GraphState, AnalysisRequest, AnalysisResult, ContentType
from src.analyzers import URLAnalyzer, ImageAnalyzer, CodeAnalyzer, ForumAnalyzer
from src.config import config
import logging

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


def input_node(state: GraphState) -> Dict[str, Any]:
    """è¾“å…¥èŠ‚ç‚¹ï¼šå¤„ç†åˆ†æè¯·æ±‚"""
    logger.info("=== ğŸ“¥ è¾“å…¥èŠ‚ç‚¹ï¼šå¤„ç†åˆ†æè¯·æ±‚ ===")
    
    analysis_requests = state.get("analysis_requests", [])
    
    if not analysis_requests:
        logger.warning("âš ï¸ æœªæä¾›åˆ†æè¯·æ±‚")
        return {
            "current_step": "input_error",
            "messages": state.get("messages", []) + ["æœªæä¾›åˆ†æè¯·æ±‚"],
        }
    
    logger.info(f"ğŸ“¥ æ”¶åˆ° {len(analysis_requests)} ä¸ªåˆ†æè¯·æ±‚")
    for i, req in enumerate(analysis_requests):
        logger.info(f"  {i+1}. ç±»å‹: {req['content_type'].value}, å†…å®¹: {req['content'][:50]}...")
    
    logger.info("âœ… è¾“å…¥èŠ‚ç‚¹å¤„ç†å®Œæˆ")
    return {
        "current_step": "input_processed",
        "messages": state.get("messages", []) + [f"å·²æ¥æ”¶ {len(analysis_requests)} ä¸ªåˆ†æè¯·æ±‚"],
        "metadata": {**state.get("metadata", {}), "input_processed": True}
    }


def analysis_node(state: GraphState) -> Dict[str, Any]:
    """åˆ†æèŠ‚ç‚¹ï¼šæ‰§è¡Œå¤šæ¨¡æ€å†…å®¹åˆ†æ"""
    logger.info("\n=== ğŸ” åˆ†æèŠ‚ç‚¹ï¼šæ‰§è¡Œå†…å®¹åˆ†æ ===")
    
    analysis_requests = state.get("analysis_requests", [])
    analysis_results = []
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è®ºå›æ•°æ®éœ€è¦å¤„ç†
    forum_data = state.get("forum_data")
    if forum_data:
        logger.info("ğŸ” æ£€æµ‹åˆ°è®ºå›æ•°æ®ï¼Œä½¿ç”¨è®ºå›åˆ†æå™¨")
        forum_analyzer = ForumAnalyzer()
        forum_result = forum_analyzer.analyze_forum(forum_data)
        analysis_results.append(forum_result)
        
        # å¦‚æœæœ‰åª’ä½“å†…å®¹éœ€è¦è¿›ä¸€æ­¥åˆ†æ
        media_requests = forum_result.get("media_requests", [])
        if media_requests:
            logger.info(f"ğŸ“ å‘ç° {len(media_requests)} ä¸ªåª’ä½“å†…å®¹éœ€è¦åˆ†æ")
            # å°†åª’ä½“è¯·æ±‚æ·»åŠ åˆ°åˆ†æé˜Ÿåˆ—
            analysis_requests.extend(media_requests)
    
    # åˆå§‹åŒ–åˆ†æå™¨
    url_analyzer = URLAnalyzer()
    image_analyzer = ImageAnalyzer()
    code_analyzer = CodeAnalyzer()
    
    for i, request in enumerate(analysis_requests):
        logger.info(f"\nğŸ” åˆ†æç¬¬ {i+1} ä¸ªå†…å®¹ ({request['content_type'].value})")
        
        try:
            if request['content_type'] == ContentType.URL:
                logger.info("ğŸŒ ä½¿ç”¨URLåˆ†æå™¨")
                result = url_analyzer.analyze_url(request['content'])
            elif request['content_type'] == ContentType.IMAGE:
                logger.info("ğŸ–¼ï¸ ä½¿ç”¨å›¾åƒåˆ†æå™¨")
                result = image_analyzer.analyze_image(request['content'])
            elif request['content_type'] == ContentType.CODE:
                # ä»contextä¸­è·å–ç¼–ç¨‹è¯­è¨€ä¿¡æ¯
                language = request.get('context', 'Unknown')
                logger.info(f"ğŸ’» ä½¿ç”¨ä»£ç åˆ†æå™¨ (è¯­è¨€: {language})")
                result = code_analyzer.analyze_code(request['content'], language)
            else:
                # æ–‡æœ¬å†…å®¹ä½¿ç”¨åŸºç¡€åˆ†æå™¨
                logger.info("ğŸ“ ä½¿ç”¨æ–‡æœ¬åˆ†æå™¨")
                analyzer = URLAnalyzer()  # å¤ç”¨URLåˆ†æå™¨çš„æ–‡æœ¬åˆ†æèƒ½åŠ›
                prompt = f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬å†…å®¹ï¼š\n{request['content']}\n\nè¯·æä¾›æ€»ç»“å’Œå…³é”®ç‚¹ã€‚"
                analysis = analyzer.analyze_with_openai(prompt)
                
                result = {
                    "content_type": ContentType.TEXT,
                    "original_content": request['content'][:100] + "...",
                    "analysis": analysis,
                    "summary": analysis[:200] + "...",
                    "key_points": analyzer._extract_key_points(analysis),
                    "confidence": 0.8
                }
            
            analysis_results.append(result)
            logger.info(f"âœ… åˆ†æå®Œæˆï¼Œç½®ä¿¡åº¦: {result['confidence']}")
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
            error_result = {
                "content_type": request['content_type'],
                "original_content": request['content'][:100],
                "analysis": f"åˆ†æå¤±è´¥: {str(e)}",
                "summary": "åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯",
                "key_points": [],
                "confidence": 0.0
            }
            analysis_results.append(error_result)
    
    logger.info(f"\nğŸ“Š å®Œæˆ {len(analysis_results)} ä¸ªå†…å®¹çš„åˆ†æ")
    logger.info("âœ… åˆ†æèŠ‚ç‚¹å¤„ç†å®Œæˆ")
    
    return {
        "current_step": "analysis_completed",
        "messages": state.get("messages", []) + [f"å®Œæˆ {len(analysis_results)} ä¸ªå†…å®¹çš„åˆ†æ"],
        "analysis_results": analysis_results,
        "metadata": {**state.get("metadata", {}), "analysis_completed": True}
    }


def summary_node(state: GraphState) -> Dict[str, Any]:
    """æ€»ç»“èŠ‚ç‚¹ï¼šç”Ÿæˆç»¼åˆæ€»ç»“å’Œå½’çº³"""
    logger.info("\n=== ğŸ“‹ æ€»ç»“èŠ‚ç‚¹ï¼šç”Ÿæˆç»¼åˆæ€»ç»“ ===")
    
    analysis_results = state.get("analysis_results", [])
    
    if not analysis_results:
        logger.warning("âš ï¸ æ²¡æœ‰åˆ†æç»“æœå¯ä»¥æ€»ç»“")
        return {
            "current_step": "summary_error",
            "messages": state.get("messages", []) + ["æ²¡æœ‰åˆ†æç»“æœå¯ä»¥æ€»ç»“"],
            "final_summary": "æ— å¯ç”¨å†…å®¹è¿›è¡Œæ€»ç»“",
            "consolidated_key_points": []
        }
    
    # æ”¶é›†æ‰€æœ‰åˆ†æç»“æœ
    all_summaries = []
    all_key_points = []
    content_types = []
    
    for result in analysis_results:
        if result['confidence'] > 0.5:  # åªåŒ…å«ç½®ä¿¡åº¦è¾ƒé«˜çš„ç»“æœ
            all_summaries.append(result['summary'])
            all_key_points.extend(result['key_points'])
            content_types.append(result['content_type'].value)
    
    logger.info(f"ğŸ“ˆ æ”¶é›†åˆ° {len(all_summaries)} ä¸ªé«˜ç½®ä¿¡åº¦æ‘˜è¦å’Œ {len(all_key_points)} ä¸ªå…³é”®ç‚¹")
    
    # ç”Ÿæˆç»¼åˆæ€»ç»“
    combined_content = "\n".join([f"- {summary}" for summary in all_summaries])
    
    prompt = f"""
    è¯·åŸºäºä»¥ä¸‹åˆ†æç»“æœç”Ÿæˆä¸€ä¸ªç»¼åˆæ€»ç»“ï¼š

    å†…å®¹ç±»å‹: {', '.join(set(content_types))}
    åˆ†æå†…å®¹:
    {combined_content}

    å…³é”®ç‚¹:
    {chr(10).join([f"- {point}" for point in all_key_points[:10]])}

    è¯·æä¾›ï¼š
    1. æ•´ä½“ä¸»é¢˜å’Œæ ¸å¿ƒè§‚ç‚¹
    2. ä¸»è¦å‘ç°å’Œæ´å¯Ÿ
    3. å®ç”¨æ€§å’Œä»·å€¼è¯„ä¼°
    4. ç»¼åˆå»ºè®®
    
    è¯·ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€æ€»ç»“ï¼Œçªå‡ºæœ€é‡è¦çš„ä¿¡æ¯ã€‚
    """
    
    try:
        # ä½¿ç”¨OpenAIç”Ÿæˆæœ€ç»ˆæ€»ç»“
        logger.info("ğŸ¤– ä½¿ç”¨OpenAIç”Ÿæˆç»¼åˆæ€»ç»“")
        analyzer = URLAnalyzer()  # å¤ç”¨åˆ†æå™¨
        final_summary = analyzer.analyze_with_openai(prompt)
        
        if "å¤±è´¥" in final_summary:
            logger.info("ğŸ”„ OpenAIå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨Gemini")
            final_summary = analyzer.analyze_with_gemini(prompt)
        
        # ç²¾é€‰å…³é”®ç‚¹ï¼ˆå»é‡å¹¶é™åˆ¶æ•°é‡ï¼‰
        unique_key_points = []
        seen_points = set()
        
        for point in all_key_points:
            cleaned_point = point.strip().lower()
            if cleaned_point not in seen_points and len(unique_key_points) < 8:
                unique_key_points.append(point.strip())
                seen_points.add(cleaned_point)
        
        logger.info(f"ğŸ“‹ ç”Ÿæˆç»¼åˆæ€»ç»“ï¼ŒåŒ…å« {len(unique_key_points)} ä¸ªå…³é”®ç‚¹")
        logger.info("âœ… æ€»ç»“èŠ‚ç‚¹å¤„ç†å®Œæˆ")
        
        return {
            "current_step": "summary_completed",
            "messages": state.get("messages", []) + ["ç”Ÿæˆç»¼åˆæ€»ç»“å®Œæˆ"],
            "final_summary": final_summary,
            "consolidated_key_points": unique_key_points,
            "metadata": {**state.get("metadata", {}), "summary_completed": True}
        }
        
    except Exception as e:
        logger.error(f"âŒ æ€»ç»“ç”Ÿæˆå¤±è´¥: {str(e)}")
        
        # é™çº§å¤„ç†ï¼šæ‰‹åŠ¨ç»„åˆæ€»ç»“
        fallback_summary = f"åˆ†æäº† {len(analysis_results)} ä¸ªå†…å®¹ï¼ŒåŒ…æ‹¬ {', '.join(set(content_types))}ã€‚"
        if all_summaries:
            fallback_summary += " ä¸»è¦å†…å®¹ï¼š" + " ".join(all_summaries[:3])
        
        logger.info("ğŸ”§ ä½¿ç”¨å¤‡ç”¨æ–¹å¼ç”Ÿæˆæ€»ç»“")
        return {
            "current_step": "summary_fallback",
            "messages": state.get("messages", []) + ["ä½¿ç”¨å¤‡ç”¨æ–¹å¼ç”Ÿæˆæ€»ç»“"],
            "final_summary": fallback_summary,
            "consolidated_key_points": all_key_points[:5],
            "metadata": {**state.get("metadata", {}), "summary_fallback": True}
        }


def output_node(state: GraphState) -> Dict[str, Any]:
    """è¾“å‡ºèŠ‚ç‚¹ï¼šæ ¼å¼åŒ–å¹¶å±•ç¤ºæœ€ç»ˆç»“æœ"""
    logger.info("\n=== ğŸ“¤ è¾“å‡ºèŠ‚ç‚¹ï¼šç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š ===")
    
    final_summary = state.get("final_summary", "æ— å¯ç”¨æ€»ç»“")
    consolidated_key_points = state.get("consolidated_key_points", [])
    analysis_results = state.get("analysis_results", [])
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    report_lines = [
        "=" * 60,
        "ğŸ“Š å¤šæ¨¡æ€å†…å®¹åˆ†ææŠ¥å‘Š",
        "=" * 60,
        "",
        "ğŸ¯ ç»¼åˆæ€»ç»“:",
        final_summary,
        "",
        "ğŸ”‘ å…³é”®è¦ç‚¹:",
    ]
    
    for i, point in enumerate(consolidated_key_points, 1):
        report_lines.append(f"  {i}. {point}")
    
    if analysis_results:
        report_lines.extend([
            "",
            "ğŸ“‹ åˆ†æè¯¦æƒ…:",
            f"  - æ€»è®¡åˆ†æ: {len(analysis_results)} ä¸ªå†…å®¹",
            f"  - æˆåŠŸåˆ†æ: {sum(1 for r in analysis_results if r['confidence'] > 0.5)} ä¸ª",
            f"  - å†…å®¹ç±»å‹: {', '.join(set(r['content_type'].value for r in analysis_results))}",
        ])
    
    report_lines.extend([
        "",
        "=" * 60,
        "âœ… åˆ†æå®Œæˆ"
    ])
    
    final_report = "\n".join(report_lines)
    logger.info(final_report)
    logger.info("âœ… è¾“å‡ºèŠ‚ç‚¹å¤„ç†å®Œæˆ")
    
    return {
        "current_step": "output_generated",
        "messages": state.get("messages", []) + ["ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šå®Œæˆ"],
        "final_report": final_report,
        "metadata": {**state.get("metadata", {}), "output_generated": True}
    }