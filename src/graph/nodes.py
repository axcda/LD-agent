from typing import Dict, Any, List
from src.graph.state import GraphState, AnalysisRequest, AnalysisResult, ContentType
from src.analyzers import URLAnalyzer, ImageAnalyzer, CodeAnalyzer, ForumAnalyzer, MCPAnalyzer, TavilyAnalyzer
from src.config import config
import logging
import os

# é…ç½®æ—¥å¿—
# ä»ç¯å¢ƒå˜é‡è·å–æ—¥å¿—çº§åˆ«ï¼Œé»˜è®¤ä¸ºINFO
log_level = os.getenv("LOG_LEVEL", "INFO").upper()
logging.basicConfig(level=getattr(logging, log_level, logging.INFO))
logger = logging.getLogger(__name__)


def input_node(state: GraphState) -> Dict[str, Any]:
    """è¾“å…¥èŠ‚ç‚¹ï¼šå¤„ç†åˆ†æè¯·æ±‚"""
    logger.info("=== ğŸ“¥ è¾“å…¥èŠ‚ç‚¹ï¼šå¤„ç†åˆ†æè¯·æ±‚ ===")
    logger.debug(f"ğŸ“¥ è¾“å…¥çŠ¶æ€: {state}")
    
    analysis_requests = state.get("analysis_requests", [])
    logger.debug(f"ğŸ“‹ åˆ†æè¯·æ±‚æ•°é‡: {len(analysis_requests)}")
    
    if not analysis_requests:
        logger.warning("âš ï¸ æœªæä¾›åˆ†æè¯·æ±‚")
        logger.debug("â†©ï¸ è¿”å›é”™è¯¯çŠ¶æ€")
        return {
            "current_step": "input_error",
            "messages": state.get("messages", []) + ["æœªæä¾›åˆ†æè¯·æ±‚"],
        }
    
    logger.info(f"ğŸ“¥ æ”¶åˆ° {len(analysis_requests)} ä¸ªåˆ†æè¯·æ±‚")
    for i, req in enumerate(analysis_requests):
        logger.info(f"  {i+1}. ç±»å‹: {req['content_type'].value}, å†…å®¹: {req['content'][:50]}...")
        logger.debug(f"     è¯¦ç»†å†…å®¹: {req}")
    
    logger.info("âœ… è¾“å…¥èŠ‚ç‚¹å¤„ç†å®Œæˆ")
    logger.debug("â†©ï¸ è¿”å›å¤„ç†ç»“æœ")
    return {
        "current_step": "input_processed",
        "messages": state.get("messages", []) + [f"å·²æ¥æ”¶ {len(analysis_requests)} ä¸ªåˆ†æè¯·æ±‚"],
        "metadata": {**state.get("metadata", {}), "input_processed": True}
    }


def analysis_node(state: GraphState) -> Dict[str, Any]:
    """åˆ†æèŠ‚ç‚¹ï¼šæ‰§è¡Œå¤šæ¨¡æ€å†…å®¹åˆ†æ"""
    logger.info("\n=== ğŸ” åˆ†æèŠ‚ç‚¹ï¼šæ‰§è¡Œå†…å®¹åˆ†æ ===")
    logger.debug(f"ğŸ” åˆ†æèŠ‚ç‚¹æ¥æ”¶çŠ¶æ€: {state}")
    
    analysis_requests = state.get("analysis_requests", [])
    analysis_results = []
    logger.debug(f"ğŸ“‹ åˆå§‹åˆ†æè¯·æ±‚æ•°é‡: {len(analysis_requests)}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è®ºå›æ•°æ®éœ€è¦å¤„ç†
    forum_data = state.get("forum_data")
    logger.debug(f"ğŸ“‚ è®ºå›æ•°æ®å­˜åœ¨: {bool(forum_data)}")
    if forum_data:
        logger.info("ğŸ” æ£€æµ‹åˆ°è®ºå›æ•°æ®ï¼Œä½¿ç”¨è®ºå›åˆ†æå™¨")
        logger.debug(f"ğŸ“‚ è®ºå›æ•°æ®è¯¦æƒ…: {forum_data}")
        forum_analyzer = ForumAnalyzer()
        logger.debug("ğŸ”§ åˆ›å»ºè®ºå›åˆ†æå™¨å®ä¾‹")
        forum_result = forum_analyzer.analyze_forum(forum_data)
        logger.debug(f"ğŸ“Š è®ºå›åˆ†æç»“æœ: {forum_result}")
        analysis_results.append(forum_result)
        
        # å¦‚æœæœ‰åª’ä½“å†…å®¹éœ€è¦è¿›ä¸€æ­¥åˆ†æ
        media_requests = forum_result.get("media_requests", [])
        logger.debug(f"ğŸ“ åª’ä½“è¯·æ±‚æ•°é‡: {len(media_requests)}")
        if media_requests:
            logger.info(f"ğŸ“ å‘ç° {len(media_requests)} ä¸ªåª’ä½“å†…å®¹éœ€è¦åˆ†æ")
            # å°†åª’ä½“è¯·æ±‚æ·»åŠ åˆ°åˆ†æé˜Ÿåˆ—
            analysis_requests.extend(media_requests)
            logger.debug(f"ğŸ“‹ æ›´æ–°ååˆ†æè¯·æ±‚æ•°é‡: {len(analysis_requests)}")
        
        # æ·»åŠ é“¾æ¥åˆ†æç»“æœåˆ°åˆ†æç»“æœä¸­
        link_analyses = forum_result.get("link_analyses", [])
        logger.debug(f"ğŸ”— é“¾æ¥åˆ†ææ•°é‡: {len(link_analyses)}")
        if link_analyses:
            logger.info(f"ğŸ”— å‘ç° {len(link_analyses)} ä¸ªé“¾æ¥åˆ†æç»“æœ")
            for link_analysis in link_analyses:
                analysis_results.append(link_analysis["analysis"])
    
    # åˆå§‹åŒ–åˆ†æå™¨
    logger.debug("ğŸ”§ åˆå§‹åŒ–åˆ†æå™¨...")
    url_analyzer = URLAnalyzer()
    image_analyzer = ImageAnalyzer()
    code_analyzer = CodeAnalyzer()
    mcp_analyzer = MCPAnalyzer()
    tavily_analyzer = TavilyAnalyzer()
    logger.debug("âœ… åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
    
    for i, request in enumerate(analysis_requests):
        logger.info(f"\nğŸ” åˆ†æç¬¬ {i+1} ä¸ªå†…å®¹ ({request['content_type'].value})")
        logger.debug(f"ğŸ“ åˆ†æè¯·æ±‚è¯¦æƒ…: {request}")
        
        try:
            # é¦–å…ˆå°è¯•ä½¿ç”¨MCPåˆ†æå™¨
            logger.info("ğŸ”§ å°è¯•ä½¿ç”¨MCPåˆ†æå™¨")
            mcp_result = mcp_analyzer.analyze_content(request['content'], request['content_type'])
            
            if mcp_result and mcp_result.get('analysis') != "MCPåˆ†æå¤±è´¥: None":
                logger.info("âœ… MCPåˆ†æå™¨æˆåŠŸè¿”å›ç»“æœ")
                result = {
                    "content_type": request['content_type'],
                    "original_content": request['content'][:100] + "...",
                    "analysis": mcp_result.get('analysis', 'æ— åˆ†æç»“æœ'),
                    "summary": mcp_result.get('analysis', 'æ— åˆ†æç»“æœ')[:200] + "...",
                    "key_points": mcp_result.get('key_points', []),
                    "confidence": 0.9,  # MCPåˆ†æå™¨ç½®ä¿¡åº¦æ›´é«˜
                    "metadata": {**mcp_result.get('metadata', {}), "analyzer": "mcp"}
                }
                logger.debug(f"ğŸ”§ MCPåˆ†æç»“æœ: {result}")
            else:
                logger.info("ğŸ”§ MCPåˆ†æå™¨ä¸å¯ç”¨ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºæœç´¢è¯·æ±‚")
                # æ£€æŸ¥æ˜¯å¦æ˜¯æœç´¢è¯·æ±‚
                if request['content_type'] == ContentType.TEXT and request['content'].startswith("search:"):
                    logger.info("ğŸ” æ£€æµ‹åˆ°æœç´¢è¯·æ±‚ï¼Œä½¿ç”¨Tavilyåˆ†æå™¨")
                    query = request['content'][7:].strip()  # ç§»é™¤"search:"å‰ç¼€
                    logger.debug(f"ğŸ” æœç´¢æŸ¥è¯¢: {query}")
                    
                    # æ‰§è¡ŒTavilyæœç´¢
                    tavily_result = tavily_analyzer.search(query)
                    logger.debug(f"ğŸ” Tavilyæœç´¢ç»“æœ: {tavily_result}")
                    
                    if tavily_result["success"]:
                        # æ ¼å¼åŒ–æœç´¢ç»“æœ
                        search_content = f"æœç´¢æŸ¥è¯¢: {query}\n\n"
                        if tavily_result.get("answer"):
                            search_content += f"ç­”æ¡ˆ: {tavily_result['answer']}\n\n"
                        
                        search_content += "æœç´¢ç»“æœ:\n"
                        for i, result in enumerate(tavily_result["results"], 1):
                            search_content += f"{i}. {result['title']}\n"
                            search_content += f"   URL: {result['url']}\n"
                            search_content += f"   å†…å®¹: {result['content'][:200]}...\n\n"
                        
                        result = {
                            "content_type": ContentType.TEXT,
                            "original_content": request['content'],
                            "analysis": search_content,
                            "summary": f"æœç´¢æŸ¥è¯¢ '{query}' çš„ç»“æœæ‘˜è¦",
                            "key_points": [f"æœç´¢ç»“æœ {i}: {r['title']}" for i, r in enumerate(tavily_result["results"], 1)],
                            "confidence": 0.85,
                            "metadata": {"analyzer": "tavily", "query": query}
                        }
                    else:
                        # æœç´¢å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ–‡æœ¬åˆ†æ
                        logger.warning(f"âŒ Tavilyæœç´¢å¤±è´¥: {tavily_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                        logger.info("ğŸ“ ä½¿ç”¨æ–‡æœ¬åˆ†æå™¨ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ")
                        analyzer = URLAnalyzer()  # å¤ç”¨URLåˆ†æå™¨çš„æ–‡æœ¬åˆ†æèƒ½åŠ›
                        prompt = f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬å†…å®¹ï¼š\n{request['content']}\n\nè¯·æä¾›æ€»ç»“å’Œå…³é”®ç‚¹ã€‚"
                        logger.debug(f"ğŸ“ å‘é€åˆ†æè¯·æ±‚åˆ°OpenAI...")
                        analysis = analyzer.analyze_with_openai(prompt)
                        logger.debug(f"ğŸ“ æ–‡æœ¬åˆ†æç»“æœ: {analysis}")
                        
                        result = {
                            "content_type": ContentType.TEXT,
                            "original_content": request['content'][:100] + "...",
                            "analysis": analysis,
                            "summary": analysis[:200] + "...",
                            "key_points": analyzer.extractKeyPoints(analysis),
                            "confidence": 0.7,
                            "metadata": {"analyzer": "fallback"}
                        }
                else:
                    logger.info("ğŸ”§ ä½¿ç”¨ä¼ ç»Ÿåˆ†æå™¨")
                if request['content_type'] == ContentType.URL:
                    logger.info("ğŸŒ ä½¿ç”¨URLåˆ†æå™¨")
                    logger.debug(f"ğŸ”— åˆ†æURL: {request['content']}")
                    result = url_analyzer.analyze_url(request['content'])
                    logger.debug(f"ğŸŒ URLåˆ†æç»“æœ: {result}")
                elif request['content_type'] == ContentType.IMAGE:
                    logger.info("ğŸ–¼ï¸ ä½¿ç”¨å›¾åƒåˆ†æå™¨")
                    logger.debug(f"ğŸ–¼ï¸ åˆ†æå›¾åƒ: {request['content']}")
                    result = image_analyzer.analyze_image(request['content'])
                    logger.debug(f"ğŸ–¼ï¸ å›¾åƒåˆ†æç»“æœ: {result}")
                elif request['content_type'] == ContentType.CODE:
                    # ä»contextä¸­è·å–ç¼–ç¨‹è¯­è¨€ä¿¡æ¯
                    language = request.get('context', 'Unknown')
                    logger.info(f"ğŸ’» ä½¿ç”¨ä»£ç åˆ†æå™¨ (è¯­è¨€: {language})")
                    logger.debug(f"ğŸ’» åˆ†æä»£ç : {request['content']}")
                    result = code_analyzer.analyze_code(request['content'], language)
                    logger.debug(f"ğŸ’» ä»£ç åˆ†æç»“æœ: {result}")
                else:
                    # æ–‡æœ¬å†…å®¹ä½¿ç”¨åŸºç¡€åˆ†æå™¨
                    logger.info("ğŸ“ ä½¿ç”¨æ–‡æœ¬åˆ†æå™¨")
                    logger.debug(f"ğŸ“ åˆ†ææ–‡æœ¬: {request['content']}")
                    analyzer = URLAnalyzer()  # å¤ç”¨URLåˆ†æå™¨çš„æ–‡æœ¬åˆ†æèƒ½åŠ›
                    prompt = f"è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬å†…å®¹ï¼š\n{request['content']}\n\nè¯·æä¾›æ€»ç»“å’Œå…³é”®ç‚¹ã€‚"
                    logger.debug(f"ğŸ“ å‘é€åˆ†æè¯·æ±‚åˆ°OpenAI...")
                    analysis = analyzer.analyze_with_openai(prompt)
                    logger.debug(f"ğŸ“ æ–‡æœ¬åˆ†æç»“æœ: {analysis}")
                    
                    result = {
                        "content_type": ContentType.TEXT,
                        "original_content": request['content'][:100] + "...",
                        "analysis": analysis,
                        "summary": analysis[:200] + "...",
                        "key_points": analyzer.extractKeyPoints(analysis),
                        "confidence": 0.8
                    }
            
            analysis_results.append(result)
            logger.info(f"âœ… åˆ†æå®Œæˆï¼Œç½®ä¿¡åº¦: {result['confidence']}")
            logger.debug(f"ğŸ“Š å½“å‰åˆ†æç»“æœæ•°é‡: {len(analysis_results)}")
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
            logger.debug(f"âŒ é”™è¯¯è¯¦æƒ…: {e}", exc_info=True)
            error_result = {
                "content_type": request['content_type'],
                "original_content": request['content'][:100],
                "analysis": f"åˆ†æå¤±è´¥: {str(e)}",
                "summary": "åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯",
                "key_points": [],
                "confidence": 0.0
            }
            analysis_results.append(error_result)
    
    logger.info(f"\nğŸ“Š å®Œæˆ {len(analysis_results)} ä¸ªå†…å®¹çš„åˆ†æ")
    logger.info("âœ… åˆ†æèŠ‚ç‚¹å¤„ç†å®Œæˆ")
    logger.debug(f"â†©ï¸ è¿”å›åˆ†æç»“æœ: {analysis_results}")
    
    return {
        "current_step": "analysis_completed",
        "messages": state.get("messages", []) + [f"å®Œæˆ {len(analysis_results)} ä¸ªå†…å®¹çš„åˆ†æ"],
        "analysis_results": analysis_results,
        "metadata": {**state.get("metadata", {}), "analysis_completed": True}
    }


def summary_node(state: GraphState) -> Dict[str, Any]:
    """æ€»ç»“èŠ‚ç‚¹ï¼šç”Ÿæˆç»¼åˆæ€»ç»“å’Œå½’çº³"""
    logger.info("\n=== ğŸ“‹ æ€»ç»“èŠ‚ç‚¹ï¼šç”Ÿæˆç»¼åˆæ€»ç»“ ===")
    logger.debug(f"ğŸ“‹ æ€»ç»“èŠ‚ç‚¹æ¥æ”¶çŠ¶æ€: {state}")
    
    analysis_results = state.get("analysis_results", [])
    logger.debug(f"ğŸ“Š åˆ†æç»“æœæ•°é‡: {len(analysis_results)}")
    
    if not analysis_results:
        logger.warning("âš ï¸ æ²¡æœ‰åˆ†æç»“æœå¯ä»¥æ€»ç»“")
        logger.debug("â†©ï¸ è¿”å›ç©ºæ€»ç»“")
        return {
            "current_step": "summary_error",
            "messages": state.get("messages", []) + ["æ²¡æœ‰åˆ†æç»“æœå¯ä»¥æ€»ç»“"],
            "final_summary": "æ— å¯ç”¨å†…å®¹è¿›è¡Œæ€»ç»“",
            "consolidated_key_points": []
        }
    
    # æ”¶é›†æ‰€æœ‰åˆ†æç»“æœ
    all_summaries = []
    all_key_points = []
    content_types = []
    
    logger.debug("ğŸ” å¼€å§‹æ”¶é›†åˆ†æç»“æœ...")
    for result in analysis_results:
        logger.debug(f"  ğŸ” å¤„ç†ç»“æœ: {result}")
        if result['confidence'] > 0.5:  # åªåŒ…å«ç½®ä¿¡åº¦è¾ƒé«˜çš„ç»“æœ
            all_summaries.append(result['summary'])
            
            # å¯¹äºå›¾åƒåˆ†æç»“æœï¼Œè¿‡æ»¤æ‰æ— ä»·å€¼çš„å…³é”®ç‚¹
            if result['content_type'] == ContentType.IMAGE:
                filtered_key_points = []
                for point in result['key_points']:
                    # è¿‡æ»¤æ‰æ— ä»·å€¼çš„å…³é”®è¯
                    if not any(keyword in point.lower() for keyword in [
                        "èµ„æºç±»å‹", "äºŒè¿›åˆ¶æ•°æ®", "ä¹±ç ", "æ— æ ‡é¢˜", "æ— é™„åŠ è¯´æ˜",
                        "ä¸Šä¸‹æ–‡", "å¯ä¿¡åº¦", "ä»·å€¼è¯„ä¼°", "å›¾ç‰‡ä¸‹è½½", "æ— æ³•åˆ†æ"
                    ]):
                        filtered_key_points.append(point)
                all_key_points.extend(filtered_key_points)
            else:
                all_key_points.extend(result['key_points'])
                
            content_types.append(result['content_type'].value)
            logger.debug(f"    âœ… æ·»åŠ é«˜ç½®ä¿¡åº¦ç»“æœ")
        else:
            logger.debug(f"    âŒ è·³è¿‡ä½ç½®ä¿¡åº¦ç»“æœ (ç½®ä¿¡åº¦: {result['confidence']})")
    
    logger.info(f"ğŸ“ˆ æ”¶é›†åˆ° {len(all_summaries)} ä¸ªé«˜ç½®ä¿¡åº¦æ‘˜è¦å’Œ {len(all_key_points)} ä¸ªå…³é”®ç‚¹")
    logger.debug(f"ğŸ“‹ æ‘˜è¦å†…å®¹: {all_summaries}")
    logger.debug(f"ğŸ”‘ å…³é”®ç‚¹: {all_key_points}")
    logger.debug(f"ğŸ·ï¸ å†…å®¹ç±»å‹: {content_types}")
    
    # ç”Ÿæˆç»¼åˆæ€»ç»“
    combined_content = "\n".join([f"- {summary}" for summary in all_summaries])
    logger.debug(f"ğŸ“ ç»„åˆå†…å®¹é•¿åº¦: {len(combined_content)}")
    
    prompt = f"""
    è¯·åŸºäºä»¥ä¸‹åˆ†æç»“æœç”Ÿæˆä¸€ä¸ªç»¼åˆæ€»ç»“ï¼š

    å†…å®¹ç±»å‹: {', '.join(set(content_types))}
    åˆ†æå†…å®¹:
    {combined_content}

    å…³é”®ç‚¹:
    {chr(10).join([f"- {point}" for point in all_key_points[:10]])}

    è¯·æä¾›ï¼š
    1. æ•´ä½“ä¸»é¢˜å’Œæ ¸å¿ƒè§‚ç‚¹
    2. ä¸»è¦å‘ç°å’Œæ´å¯Ÿ
    3. å®ç”¨æ€§å’Œä»·å€¼è¯„ä¼°
    4. ç»¼åˆå»ºè®®
    
    è¯·ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€æ€»ç»“ï¼Œçªå‡ºæœ€é‡è¦çš„ä¿¡æ¯ã€‚
    """
    logger.debug("ğŸ¤– å‡†å¤‡ç”Ÿæˆç»¼åˆæ€»ç»“...")
    
    try:
        # ä½¿ç”¨OpenAIç”Ÿæˆæœ€ç»ˆæ€»ç»“
        logger.info("ğŸ¤– ä½¿ç”¨OpenAIç”Ÿæˆç»¼åˆæ€»ç»“")
        logger.debug("ğŸ”§ åˆ›å»ºURLåˆ†æå™¨å®ä¾‹...")
        analyzer = URLAnalyzer()  # å¤ç”¨åˆ†æå™¨
        logger.debug("ğŸ“¤ å‘é€è¯·æ±‚åˆ°OpenAI...")
        final_summary = analyzer.analyzeWithOpenai(prompt)
        logger.debug(f"ğŸ“¥ OpenAIå“åº”: {final_summary[:100]}...")
        
        if "å¤±è´¥" in final_summary:
            logger.info("ğŸ”„ OpenAIå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨Gemini")
            logger.debug("ğŸ“¤ å‘é€è¯·æ±‚åˆ°Gemini...")
            final_summary = analyzer.analyzeWithGemini(prompt)
            logger.debug(f"ğŸ“¥ Geminiå“åº”: {final_summary[:100]}...")
        
        # ç²¾é€‰å…³é”®ç‚¹ï¼ˆå»é‡å¹¶é™åˆ¶æ•°é‡ï¼‰
        unique_key_points = []
        seen_points = set()
        logger.debug("ğŸ“‹ å¼€å§‹ç²¾é€‰å…³é”®ç‚¹...")
        
        for point in all_key_points:
            cleaned_point = point.strip().lower()
            # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨ç›¸ä¼¼çš„å…³é”®ç‚¹
            is_duplicate = False
            for seen_point in seen_points:
                # å¦‚æœä¸¤ä¸ªå…³é”®ç‚¹çš„ç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œåˆ™è®¤ä¸ºæ˜¯é‡å¤çš„
                if len(cleaned_point) > 10 and len(seen_point) > 10:
                    # å¯¹äºè¾ƒé•¿çš„å…³é”®ç‚¹ï¼Œä½¿ç”¨æ›´å®½æ¾çš„å»é‡ç­–ç•¥
                    if cleaned_point in seen_point or seen_point in cleaned_point:
                        is_duplicate = True
                        break
                else:
                    # å¯¹äºè¾ƒçŸ­çš„å…³é”®ç‚¹ï¼Œä½¿ç”¨ä¸¥æ ¼çš„å»é‡ç­–ç•¥
                    if cleaned_point == seen_point:
                        is_duplicate = True
                        break
            
            if not is_duplicate and len(unique_key_points) < 10:  # å¢åŠ å…³é”®ç‚¹æ•°é‡é™åˆ¶
                unique_key_points.append(point.strip())
                seen_points.add(cleaned_point)
                logger.debug(f"  âœ… æ·»åŠ å…³é”®ç‚¹: {point.strip()}")
            else:
                logger.debug(f"  âŒ è·³è¿‡å…³é”®ç‚¹: {point.strip()}")
        
        logger.info(f"ğŸ“‹ ç”Ÿæˆç»¼åˆæ€»ç»“ï¼ŒåŒ…å« {len(unique_key_points)} ä¸ªå…³é”®ç‚¹")
        logger.info("âœ… æ€»ç»“èŠ‚ç‚¹å¤„ç†å®Œæˆ")
        logger.debug(f"â†©ï¸ è¿”å›æ€»ç»“ç»“æœ")
        
        return {
            "current_step": "summary_completed",
            "messages": state.get("messages", []) + ["ç”Ÿæˆç»¼åˆæ€»ç»“å®Œæˆ"],
            "final_summary": final_summary,
            "consolidated_key_points": unique_key_points,
            "metadata": {**state.get("metadata", {}), "summary_completed": True}
        }
        
    except Exception as e:
        logger.error(f"âŒ æ€»ç»“ç”Ÿæˆå¤±è´¥: {str(e)}")
        logger.debug(f"âŒ é”™è¯¯è¯¦æƒ…: {e}", exc_info=True)
        
        # é™çº§å¤„ç†ï¼šæ‰‹åŠ¨ç»„åˆæ€»ç»“
        fallback_summary = f"åˆ†æäº† {len(analysis_results)} ä¸ªå†…å®¹ï¼ŒåŒ…æ‹¬ {', '.join(set(content_types))}ã€‚"
        if all_summaries:
            fallback_summary += " ä¸»è¦å†…å®¹ï¼š" + " ".join(all_summaries[:3])
        
        logger.info("ğŸ”§ ä½¿ç”¨å¤‡ç”¨æ–¹å¼ç”Ÿæˆæ€»ç»“")
        logger.debug(f"â†©ï¸ è¿”å›å¤‡ç”¨æ€»ç»“ç»“æœ")
        return {
            "current_step": "summary_fallback",
            "messages": state.get("messages", []) + ["ä½¿ç”¨å¤‡ç”¨æ–¹å¼ç”Ÿæˆæ€»ç»“"],
            "final_summary": fallback_summary,
            "consolidated_key_points": all_key_points[:5],
            "metadata": {**state.get("metadata", {}), "summary_fallback": True}
        }


def output_node(state: GraphState) -> Dict[str, Any]:
    """è¾“å‡ºèŠ‚ç‚¹ï¼šæ ¼å¼åŒ–å¹¶å±•ç¤ºæœ€ç»ˆç»“æœ"""
    logger.info("\n=== ğŸ“¤ è¾“å‡ºèŠ‚ç‚¹ï¼šç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š ===")
    logger.debug(f"ğŸ“¤ è¾“å‡ºèŠ‚ç‚¹æ¥æ”¶çŠ¶æ€: {state}")
    
    final_summary = state.get("final_summary", "æ— å¯ç”¨æ€»ç»“")
    consolidated_key_points = state.get("consolidated_key_points", [])
    analysis_results = state.get("analysis_results", [])
    
    logger.debug(f"ğŸ“ æœ€ç»ˆæ€»ç»“: {final_summary[:100]}...")
    logger.debug(f"ğŸ”‘ å…³é”®ç‚¹æ•°é‡: {len(consolidated_key_points)}")
    logger.debug(f"ğŸ“Š åˆ†æç»“æœæ•°é‡: {len(analysis_results)}")
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    logger.debug("ğŸ“„ å¼€å§‹ç”ŸæˆæŠ¥å‘Š...")
    report_lines = [
        "=" * 60,
        "ğŸ“Š å¤šæ¨¡æ€å†…å®¹åˆ†ææŠ¥å‘Š",
        "=" * 60,
        "",
        "ğŸ¯ ç»¼åˆæ€»ç»“:",
        final_summary,
        "",
        "ğŸ”‘ å…³é”®è¦ç‚¹:",
    ]
    logger.debug("ğŸ“ æ·»åŠ å…³é”®è¦ç‚¹...")
    
    for i, point in enumerate(consolidated_key_points, 1):
        report_lines.append(f"  {i}. {point}")
        logger.debug(f"  {i}. {point}")
    
    if analysis_results:
        logger.debug("ğŸ“‹ æ·»åŠ åˆ†æè¯¦æƒ…...")
        report_lines.extend([
            "",
            "ğŸ“‹ åˆ†æè¯¦æƒ…:",
            f"  - æ€»è®¡åˆ†æ: {len(analysis_results)} ä¸ªå†…å®¹",
            f"  - æˆåŠŸåˆ†æ: {sum(1 for r in analysis_results if r['confidence'] > 0.5)} ä¸ª",
            f"  - å†…å®¹ç±»å‹: {', '.join(set(r['content_type'].value for r in analysis_results))}",
        ])
    
    report_lines.extend([
        "",
        "=" * 60,
        "âœ… åˆ†æå®Œæˆ"
    ])
    
    final_report = "\n".join(report_lines)
    logger.info(final_report)
    logger.info("âœ… è¾“å‡ºèŠ‚ç‚¹å¤„ç†å®Œæˆ")
    logger.debug("â†©ï¸ è¿”å›è¾“å‡ºç»“æœ")
    
    return {
        "current_step": "output_generated",
        "messages": state.get("messages", []) + ["ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šå®Œæˆ"],
        "final_report": final_report,
        "metadata": {**state.get("metadata", {}), "output_generated": True}
    }