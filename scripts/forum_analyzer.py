#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®ºå›åˆ†æè„šæœ¬ - å‘½ä»¤è¡Œå·¥å…·
"""

import sys
import os
import json
import argparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from src.analyzers.forum_analyzer import ForumAnalyzer
from src.graph.state import ContentType


def load_forum_data(file_path: str) -> dict:
    """åŠ è½½è®ºå›æ•°æ®"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def save_analysis_result(result: dict, output_path: str):
    """ä¿å­˜åˆ†æç»“æœ"""
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è®ºå›å†…å®¹åˆ†æå·¥å…·')
    parser.add_argument('input_file', help='è¾“å…¥çš„è®ºå›æ•°æ®JSONæ–‡ä»¶')
    parser.add_argument('-o', '--output', help='è¾“å‡ºåˆ†æç»“æœçš„JSONæ–‡ä»¶')
    parser.add_argument('--verbose', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')
    
    args = parser.parse_args()
    
    try:
        # åŠ è½½è®ºå›æ•°æ®
        print(f"ğŸ“‚ åŠ è½½è®ºå›æ•°æ®: {args.input_file}")
        forum_data = load_forum_data(args.input_file)
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {forum_data.get('topicTitle', '')}")
        
        # åˆ›å»ºåˆ†æå™¨å¹¶æ‰§è¡Œåˆ†æ
        print("ğŸ” å¼€å§‹åˆ†æ...")
        analyzer = ForumAnalyzer()
        result = analyzer.analyze_forum(forum_data)
        
        if result and result.get("confidence", 0) > 0.5:
            print("âœ… åˆ†æå®Œæˆ!")
            
            # æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
            print(f"ç½®ä¿¡åº¦: {result['confidence']:.2f}")
            print(f"æ‘˜è¦: {result['summary'][:100]}...")
            
            if args.verbose:
                print("\nğŸ”‘ å…³é”®è¦ç‚¹:")
                for i, point in enumerate(result["key_points"][:10], 1):
                    print(f"  {i}. {point}")
                
                metadata = result.get("metadata", {})
                if metadata:
                    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
                    print(f"  - æ€»å¸–å­æ•°: {metadata.get('total_posts', 0)}")
                    print(f"  - å‚ä¸ç”¨æˆ·: {metadata.get('users_count', 0)}äºº")
                    print(f"  - å¤–éƒ¨é“¾æ¥: {metadata.get('links_count', 0)}ä¸ª")
                    print(f"  - å›¾ç‰‡å†…å®¹: {metadata.get('images_count', 0)}å¼ ")
            
            # ä¿å­˜ç»“æœ
            if args.output:
                save_analysis_result(result, args.output)
                print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
            else:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œæ˜¾ç¤ºç®€è¦ç»“æœ
                print(f"\nğŸ“‹ åˆ†æç»“æœ:")
                print(f"  - ä¸»é¢˜: {result.get('original_content', '')}")
                print(f"  - å…³é”®ç‚¹æ•°: {len(result.get('key_points', []))}")
                print(f"  - åª’ä½“è¯·æ±‚æ•°: {len(result.get('media_requests', []))}")
                
        else:
            print(f"âŒ åˆ†æå¤±è´¥: {result.get('analysis', 'æœªçŸ¥é”™è¯¯')}")
            sys.exit(1)
            
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {args.input_file}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ JSONè§£æé”™è¯¯: {str(e)}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()